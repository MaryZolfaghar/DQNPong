{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner(nn.Module):\n",
    "    def __init__(self, env, num_frames, batch_size, gamma, replay_buffer):\n",
    "        super(QLearner, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.num_frames = num_frames\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.env = env\n",
    "        self.input_shape = self.env.observation_space.shape\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "            return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        action = []\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), requires_grad=True)\n",
    "            ######## YOUR CODE HERE! ########\n",
    "            # TODO: Given state, you should write code to get the Q value and chosen action\n",
    "            # Complete the R.H.S. of the following 2 lines and uncomment them\n",
    "            q_value = self.forward(state)\n",
    "            action = torch.argmax(q_value)\n",
    "            ######## YOUR CODE HERE! ########\n",
    "        else:\n",
    "            action = random.randrange(self.env.action_space.n)\n",
    "        return action\n",
    "        \n",
    "def compute_td_loss(model, batch_size, gamma, replay_buffer):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    state = Variable(torch.FloatTensor(np.float32(state)), requires_grad=True)\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), requires_grad=True)\n",
    "    action = Variable(torch.LongTensor(action))\n",
    "    reward = Variable(torch.FloatTensor(reward))\n",
    "    done = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    ######## YOUR CODE HERE! ########\n",
    "    # TODO: Implement the Temporal Difference Loss\n",
    "    q_value = model.forward(state)\n",
    "    next_q_value = model.forward(next_state)\n",
    "    \n",
    "    target = reward + gamma*torch.max(next_q_value)\n",
    "    current = [q_value[ii,act] for ii, act in enumerate(action)]\n",
    "    current = Variable(torch.FloatTensor(np.float32(current)), requires_grad=True)\n",
    "    \n",
    "    loss = torch.sqrt(torch.mean((current - target)**2))\n",
    "    ######## YOUR CODE HERE! ########\n",
    "    return loss\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity) #Returns a new deque object initialized left-to-right\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        ######## YOUR CODE HERE! ########\n",
    "        # TODO: Randomly sampling data with specific batch size from the buffer\n",
    "        # Hint: you may use the python library \"random\".\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "#         state  = []\n",
    "#         action = []\n",
    "#         reward = []\n",
    "#         next_state = []\n",
    "#         done = []\n",
    "#         for sample in batch:\n",
    "#             state.append(sample[0])\n",
    "#             action.append(sample[1])\n",
    "#             reward.append(sample[2])\n",
    "#             next_state.append(sample[3])\n",
    "#             done.append(sample[4])\n",
    "        \n",
    "        # If you are not familiar with the \"deque\" python library, please google it.\n",
    "        ######## YOUR CODE HERE! ########\n",
    "        return batch\n",
    "#         return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=torch.tensor([[ 0.0616, -1.5050, -1.2542,  1.0467, -1.6583, -0.1956],\n",
    "        [ 0.1444, -1.4500, -1.2947,  1.0930, -1.7403, -0.2409],\n",
    "        [ 0.0342, -1.5723, -1.2624,  1.1574, -1.6852, -0.0689],\n",
    "        [ 0.0517, -1.5105, -1.2464,  1.1025, -1.6326, -0.1251],\n",
    "        [ 0.0201, -1.5424, -1.2026,  1.1654, -1.7247, -0.1792],\n",
    "        [ 0.0877, -1.4870, -1.2158,  1.0617, -1.6393, -0.1004],\n",
    "        [ 0.0881, -1.4855, -1.2736,  1.1042, -1.5769, -0.1997],\n",
    "        [ 0.1076, -1.4818, -1.2331,  1.0620, -1.6394, -0.1209],\n",
    "        [ 0.1458, -1.5549, -1.1906,  1.1144, -1.6783, -0.0942],\n",
    "        [ 0.0267, -1.5309, -1.3258,  1.0811, -1.6813, -0.1498],\n",
    "        [ 0.0187, -1.5255, -1.3182,  1.0463, -1.6692, -0.0902],\n",
    "        [ 0.0642, -1.6106, -1.2788,  1.0466, -1.6537, -0.0168],\n",
    "        [-0.0481, -1.5233, -1.2429,  1.0540, -1.6389, -0.1518],\n",
    "        [-0.0229, -1.4506, -1.2753,  0.9652, -1.6470, -0.2397],\n",
    "        [ 0.0108, -1.5544, -1.1406,  1.1781, -1.7532, -0.1468],\n",
    "        [ 0.1174, -1.5528, -1.2626,  1.0589, -1.6256, -0.1757],\n",
    "        [ 0.0499, -1.5252, -1.2493,  1.1104, -1.6591, -0.0693],\n",
    "        [ 0.0906, -1.5743, -1.2251,  1.0825, -1.6204, -0.0840],\n",
    "        [ 0.0260, -1.5498, -1.2999,  1.1594, -1.6374, -0.1158],\n",
    "        [ 0.0567, -1.4924, -1.2527,  1.0692, -1.6544, -0.1253],\n",
    "        [ 0.0356, -1.5186, -1.3110,  1.1103, -1.7097, -0.0847],\n",
    "        [ 0.0463, -1.5214, -1.2331,  1.0992, -1.6376, -0.1251],\n",
    "        [ 0.0372, -1.4996, -1.2329,  1.0936, -1.6099, -0.0969],\n",
    "        [ 0.0567, -1.5007, -1.2477,  1.0955, -1.6199, -0.1252],\n",
    "        [ 0.0213, -1.5079, -1.2459,  1.1022, -1.6472, -0.1064],\n",
    "        [ 0.0600, -1.4948, -1.2267,  1.0692, -1.6599, -0.1107],\n",
    "        [ 0.0256, -1.5184, -1.2491,  1.1132, -1.6322, -0.1291],\n",
    "        [ 0.1561, -1.5188, -1.2421,  1.1181, -1.6393, -0.1787],\n",
    "        [ 0.0355, -1.5439, -1.2369,  1.1085, -1.6272, -0.1101],\n",
    "        [ 0.0417, -1.4993, -1.3280,  1.0897, -1.6234, -0.1404],\n",
    "        [ 0.0097, -1.5227, -1.2437,  1.0888, -1.6143, -0.1501],\n",
    "        [ 0.0409, -1.6244, -1.4683,  1.1526, -1.8225, -0.1008]]) \n",
    "print(max(aa[1]))\n",
    "\n",
    "print(max(aa[1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run DQN Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Wrapper.layers import *\n",
    "from Wrapper.wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "import math, random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# from dqn import QLearner, compute_td_loss, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env)\n",
    "env = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot observation of env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_frames = 1000000\n",
    "# batch_size = 32\n",
    "# gamma = 0.99\n",
    "    \n",
    "# replay_initial = 10000\n",
    "# replay_buffer = ReplayBuffer(100000)\n",
    "# model = QLearner(env, num_frames, batch_size, gamma, replay_buffer)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "# if USE_CUDA:\n",
    "#     model = model.cuda()\n",
    "\n",
    "# epsilon_start = 1.0\n",
    "# epsilon_final = 0.01\n",
    "# epsilon_decay = 30000\n",
    "# epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "# losses = []\n",
    "# all_rewards = []\n",
    "# episode_reward = 0\n",
    "\n",
    "# state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 10\n",
    "batch_size = 3\n",
    "gamma = 0.99\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "\n",
    "model = QLearner(env, num_frames, batch_size, gamma, replay_buffer)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "replay_buffer = ReplayBuffer(100)\n",
    "for ii in range(100):\n",
    "        epsilon = epsilon_by_frame(frame_idx)\n",
    "        action = model.act(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Transition = namedtuple('Transition', \n",
    "                        ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "batch = zip(*transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 1000000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "    \n",
    "replay_initial = 10000\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "\n",
    "model = QLearner(env, num_frames, batch_size, gamma, replay_buffer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_idx in range(1, num_frames + 1):\n",
    "\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "\n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(model, batch_size, gamma, replay_buffer)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "    if frame_idx % 10000 == 0 and len(replay_buffer) <= replay_initial:\n",
    "        print('#Frame: %d, preparing replay buffer' % frame_idx)\n",
    "        print(len(replay_buffer))\n",
    "    if frame_idx % 10000 == 0 and len(replay_buffer) > replay_initial:\n",
    "        print('#Frame: %d, Loss: %f' % (frame_idx, np.mean(losses)))\n",
    "        print('Last-10 average reward: %f' % np.mean(all_rewards[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearnigEEG",
   "language": "python",
   "name": "deeplearnigeeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
