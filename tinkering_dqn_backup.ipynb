{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "print(USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner(nn.Module):\n",
    "    def __init__(self, env, args, replay_buffer):\n",
    "        super(QLearner, self).__init__()\n",
    "\n",
    "        self.batch_size = args.batch_size\n",
    "        self.gamma = args.gamma\n",
    "        self.num_frames = args.num_frames\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.env = env\n",
    "        self.input_shape = self.env.observation_space.shape\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.N = args.N\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def feature_size(self):\n",
    "            return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        action = []\n",
    "\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), \\\n",
    "                               requires_grad=True)\n",
    "            ######## YOUR CODE HERE! ########\n",
    "            # TODO: Given state, you should write code to get the Q value and chosen action\n",
    "            # Complete the R.H.S. of the following 2 lines and uncomment them\n",
    "            q_value = self.forward(state)\n",
    "            action = torch.argmax(q_value)\n",
    "            ######## YOUR CODE HERE! ########\n",
    "        else:\n",
    "            action = random.randrange(self.env.action_space.n)\n",
    "        return action\n",
    "        \n",
    "def compute_td_loss(model, batch_size, gamma, replay_buffer):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    state = Variable(torch.FloatTensor(np.float32(state)), requires_grad=True)\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), requires_grad=True)\n",
    "    action = Variable(torch.LongTensor(action))\n",
    "    reward = Variable(torch.FloatTensor(reward))\n",
    "    done = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    ######## YOUR CODE HERE! ########\n",
    "    # TODO: Implement the Temporal Difference Loss\n",
    "    q_value = model.forward(state)\n",
    "    next_q_value = model.forward(next_state)\n",
    "    \n",
    "    target = reward + gamma*torch.max(next_q_value)\n",
    "    current = [q_value[ii,act] for ii, act in enumerate(action)]\n",
    "    current = Variable(torch.FloatTensor(np.float32(current)), requires_grad=True)\n",
    "    \n",
    "    loss = torch.sqrt(torch.mean((current - target)**2))\n",
    "    ######## YOUR CODE HERE! ########\n",
    "    return loss\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        #Returns a new deque object initialized left-to-right\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ######## YOUR CODE HERE! ########\n",
    "        # TODO: Randomly sampling data with specific batch size from the buffer\n",
    "        # Hint: you may use the python library \"random\".\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "#         state  = []\n",
    "#         action = []\n",
    "#         reward = []\n",
    "#         next_state = []\n",
    "#         done = []\n",
    "#         for sample in batch:\n",
    "#             state.append(sample[0])\n",
    "#             action.append(sample[1])\n",
    "#             reward.append(sample[2])\n",
    "#             next_state.append(sample[3])\n",
    "#             done.append(sample[4])\n",
    "\n",
    "        # If you are not familiar with the \"deque\" python library, please google it.\n",
    "        ######## YOUR CODE HERE! ########\n",
    "        return batch\n",
    "#         return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.seed=1\n",
    "        self.batch_size = 32\n",
    "        self.num_frames = 1000000\n",
    "        self.gamma = 0.99\n",
    "        self.epsilon_start = 1.0\n",
    "        self.epsilon_final = 0.01\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.N = 1\n",
    "        self.optimizer = 'Adam'\n",
    "        self.lr = 1e-4\n",
    "        self.capacity = 100000\n",
    "        self.save_result_path = '../results/DQN/results.npy'\n",
    "        self.save_model_path = '../results/DQN/weights_only.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0930)\n",
      "tensor(1.0930)\n"
     ]
    }
   ],
   "source": [
    "aa=torch.tensor([[ 0.0616, -1.5050, -1.2542,  1.0467, -1.6583, -0.1956],\n",
    "        [ 0.1444, -1.4500, -1.2947,  1.0930, -1.7403, -0.2409],\n",
    "        [ 0.0342, -1.5723, -1.2624,  1.1574, -1.6852, -0.0689],\n",
    "        [ 0.0517, -1.5105, -1.2464,  1.1025, -1.6326, -0.1251],\n",
    "        [ 0.0201, -1.5424, -1.2026,  1.1654, -1.7247, -0.1792],\n",
    "        [ 0.0877, -1.4870, -1.2158,  1.0617, -1.6393, -0.1004],\n",
    "        [ 0.0881, -1.4855, -1.2736,  1.1042, -1.5769, -0.1997],\n",
    "        [ 0.1076, -1.4818, -1.2331,  1.0620, -1.6394, -0.1209],\n",
    "        [ 0.1458, -1.5549, -1.1906,  1.1144, -1.6783, -0.0942],\n",
    "        [ 0.0267, -1.5309, -1.3258,  1.0811, -1.6813, -0.1498],\n",
    "        [ 0.0187, -1.5255, -1.3182,  1.0463, -1.6692, -0.0902],\n",
    "        [ 0.0642, -1.6106, -1.2788,  1.0466, -1.6537, -0.0168],\n",
    "        [-0.0481, -1.5233, -1.2429,  1.0540, -1.6389, -0.1518],\n",
    "        [-0.0229, -1.4506, -1.2753,  0.9652, -1.6470, -0.2397],\n",
    "        [ 0.0108, -1.5544, -1.1406,  1.1781, -1.7532, -0.1468],\n",
    "        [ 0.1174, -1.5528, -1.2626,  1.0589, -1.6256, -0.1757],\n",
    "        [ 0.0499, -1.5252, -1.2493,  1.1104, -1.6591, -0.0693],\n",
    "        [ 0.0906, -1.5743, -1.2251,  1.0825, -1.6204, -0.0840],\n",
    "        [ 0.0260, -1.5498, -1.2999,  1.1594, -1.6374, -0.1158],\n",
    "        [ 0.0567, -1.4924, -1.2527,  1.0692, -1.6544, -0.1253],\n",
    "        [ 0.0356, -1.5186, -1.3110,  1.1103, -1.7097, -0.0847],\n",
    "        [ 0.0463, -1.5214, -1.2331,  1.0992, -1.6376, -0.1251],\n",
    "        [ 0.0372, -1.4996, -1.2329,  1.0936, -1.6099, -0.0969],\n",
    "        [ 0.0567, -1.5007, -1.2477,  1.0955, -1.6199, -0.1252],\n",
    "        [ 0.0213, -1.5079, -1.2459,  1.1022, -1.6472, -0.1064],\n",
    "        [ 0.0600, -1.4948, -1.2267,  1.0692, -1.6599, -0.1107],\n",
    "        [ 0.0256, -1.5184, -1.2491,  1.1132, -1.6322, -0.1291],\n",
    "        [ 0.1561, -1.5188, -1.2421,  1.1181, -1.6393, -0.1787],\n",
    "        [ 0.0355, -1.5439, -1.2369,  1.1085, -1.6272, -0.1101],\n",
    "        [ 0.0417, -1.4993, -1.3280,  1.0897, -1.6234, -0.1404],\n",
    "        [ 0.0097, -1.5227, -1.2437,  1.0888, -1.6143, -0.1501],\n",
    "        [ 0.0409, -1.6244, -1.4683,  1.1526, -1.8225, -0.1008]]) \n",
    "print(max(aa[1]))\n",
    "\n",
    "print(max(aa[1,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run DQN Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Wrapper.layers import *\n",
    "from Wrapper.wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "import math, random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# from dqn import QLearner, compute_td_loss, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env, frame_stack=False)\n",
    "env = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 84, 84)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = env.reset()\n",
    "(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot observation of env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_frames = 1000000\n",
    "# batch_size = 32\n",
    "# gamma = 0.99\n",
    "    \n",
    "# replay_initial = 10000\n",
    "# replay_buffer = ReplayBuffer(100000)\n",
    "# model = QLearner(env, num_frames, batch_size, gamma, replay_buffer)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "# if USE_CUDA:\n",
    "#     model = model.cuda()\n",
    "\n",
    "# epsilon_start = 1.0\n",
    "# epsilon_final = 0.01\n",
    "# epsilon_decay = 30000\n",
    "# epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "# losses = []\n",
    "# all_rewards = []\n",
    "# episode_reward = 0\n",
    "\n",
    "# state = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some tinkering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize target q function and q function\n",
    "model_Q = QLearner(env, args, replay_buffer)\n",
    "model_target_Q = QLearner(env, args, replay_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9933, 0.9638, 0.8908, 0.5669, 0.9713]])\n",
      "tensor([[0.9866, 0.9288, 0.7936, 0.3214, 0.9434]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1,5)\n",
    "print(a)\n",
    "print(a**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 10\n",
    "batch_size = 3\n",
    "gamma = 0.99\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "replay_buffer = ReplayBuffer(100)\n",
    "# model = QLearner(env, num_frames, batch_size, gamma, replay_buffer)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "for frame_idx in range(100):\n",
    "        epsilon = epsilon_by_frame(frame_idx)\n",
    "        action = model_Q.act(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "transition = replay_buffer.sample(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 84, 84)\n"
     ]
    }
   ],
   "source": [
    "a , b , c , d, e = zip(*transition)\n",
    "print(np.shape(transition[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "state = Variable(torch.FloatTensor(np.float32(state)), requires_grad=True)\n",
    "next_state = Variable(torch.FloatTensor(np.float32(next_state)), requires_grad=True)\n",
    "action = Variable(torch.LongTensor(action))\n",
    "reward = Variable(torch.FloatTensor(reward))\n",
    "done = Variable(torch.FloatTensor(done))\n",
    "print(done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(6)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if USE_CUDA:\n",
    "#     action = action.cuda()\n",
    "#     reward = reward.cuda()\n",
    "# ######## YOUR CODE HERE! ########\n",
    "# # TODO: Implement the Temporal Difference Loss\n",
    "\n",
    "# # Compute current Q value, q_func takes only state and output value for every state-action pair\n",
    "# # We choose Q based on action taken.\n",
    "# current_q_value = model_Q.forward(state)\n",
    "# # Compute next Q value based on which action gives max Q values\n",
    "# # Detach variable from the current graph since we don't want gradients for next Q to propagated\n",
    "# with torch.no_grad():\n",
    "#     next_q_value = model_target_Q.forward(next_state).detach()\n",
    "#     target_q_val = reward + ( (gamma**args.N)* (np.max(next_q_value.detach().cpu().numpy())) * (1-done))\n",
    "\n",
    "# print(current_q_value.shape)\n",
    "# print(target_q_val.shape)\n",
    "# # Compute Bellman error\n",
    "# loss = torch.mean((target_q_val - current_q_value)**2)\n",
    "\n",
    "# # loss = torch.nn.SmoothL1Loss(target_q_val, current_q_value)\n",
    "\n",
    "# # Not sure what this is about\n",
    "# # # clip the bellman error between [-1 , 1]\n",
    "# # bellman_error = loss\n",
    "# # clipped_bellman_error = bellman_error.clamp(-1, 1)\n",
    "# # # Note: clipped_bellman_delta * -1 will be right gradient\n",
    "# # d_error = clipped_bellman_error * -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 0, 2])\n",
      "torch.Size([3, 6])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "if USE_CUDA:\n",
    "    action = action.cuda()\n",
    "    reward = reward.cuda()\n",
    "print(action)\n",
    "current_q = model_Q.forward(state)\n",
    "current_q_value = current_q.gather(1, action.view(-1,1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    next_q_value = model_target_Q.forward(next_state).detach()\n",
    "    target_q_val = reward + ( (gamma**args.N)* (np.max(next_q_value.detach().cpu().numpy())) * (1-done))\n",
    "target_q_val = target_q_val.view(-1,1)\n",
    "print(current_q.shape)\n",
    "print(current_q_value.shape)\n",
    "print(target_q_val.shape)\n",
    "# loss = torch.mean((target_q_val - current_q_value)**2)\n",
    "loss = torch.nn.SmoothL1Loss(target_q_val, current_q_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.8305, 0.8839, 0.1926])\n",
      "1\n",
      "torch.Size([])\n",
      "tensor([1, 2])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-152-7f7a875835aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3)\n",
    "print(a)\n",
    "b = torch.argmax(a)\n",
    "print(b.item())\n",
    "print(b.size())\n",
    "c = torch.from_numpy(np.array([1,2]))\n",
    "print (c)\n",
    "print(int(c.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import namedtuple\n",
    "# Transition = namedtuple('Transition', \n",
    "#                         ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "# batch = zip(*transitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2178)\n",
      "tensor(0.0020)\n",
      "tensor(0.0819)\n",
      "tensor(2.9654e-05)\n",
      "tensor([[0.4667, 0.0447],\n",
      "        [0.2862, 0.0054]])\n",
      "tensor([[2.1784e-01, 1.9990e-03],\n",
      "        [8.1929e-02, 2.9654e-05]])\n",
      "tensor([[0.2306, 0.0211],\n",
      "        [0.1352, 0.0128]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((2,2))\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        print(a[i,j]*a[i,j])\n",
    "print(a)\n",
    "print(a**2)\n",
    "print(a@a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep Q run with default params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 1000000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "    \n",
    "replay_initial = 10000\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "\n",
    "model = QLearner(env, num_frames, batch_size, gamma, replay_buffer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for frame_idx in range(1, num_frames + 1):\n",
    "\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "\n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(model, batch_size, gamma, replay_buffer)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "    if frame_idx % 10000 == 0 and len(replay_buffer) <= replay_initial:\n",
    "        print('#Frame: %d, preparing replay buffer' % frame_idx)\n",
    "        print(len(replay_buffer))\n",
    "    if frame_idx % 10000 == 0 and len(replay_buffer) > replay_initial:\n",
    "        print('#Frame: %d, Loss: %f' % (frame_idx, np.mean(losses)))\n",
    "        print('Last-10 average reward: %f' % np.mean(all_rewards[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
