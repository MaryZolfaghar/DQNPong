{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import math, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner(nn.Module):\n",
    "    def __init__(self, env, num_frames, batch_size, gamma, replay_buffer):\n",
    "        super(QLearner, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.num_frames = num_frames\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.env = env\n",
    "        self.input_shape = self.env.observation_space.shape\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "            return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        action = []\n",
    "        \n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), requires_grad=True)\n",
    "            ######## YOUR CODE HERE! ########\n",
    "            # TODO: Given state, you should write code to get the Q value and chosen action\n",
    "            # Complete the R.H.S. of the following 2 lines and uncomment them\n",
    "            q_value = self.forward(state)\n",
    "            action = torch.argmax(q_value)\n",
    "            ######## YOUR CODE HERE! ########\n",
    "        else:\n",
    "            action = random.randrange(self.env.action_space.n)\n",
    "        return action\n",
    "        \n",
    "def compute_td_loss(model, batch_size, gamma, replay_buffer):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "    state = Variable(torch.FloatTensor(np.float32(state)), requires_grad=True)\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), requires_grad=True)\n",
    "    action = Variable(torch.LongTensor(action))\n",
    "    reward = Variable(torch.FloatTensor(reward))\n",
    "    done = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    ######## YOUR CODE HERE! ########\n",
    "    # TODO: Implement the Temporal Difference Loss\n",
    "    q_value = model.forward(state)\n",
    "    next_q_value = model.forward(next_state)\n",
    "    \n",
    "    target = reward + gamma*torch.max(next_q_value)\n",
    "    current = [q_value[ii,act] for ii, act in enumerate(action)]\n",
    "    current = Variable(torch.FloatTensor(np.float32(current)), requires_grad=True)\n",
    "    \n",
    "    loss = torch.sqrt(torch.mean((current - target)**2))\n",
    "    ######## YOUR CODE HERE! ########\n",
    "    return loss\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity) #Returns a new deque object initialized left-to-right\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        ######## YOUR CODE HERE! ########\n",
    "        # TODO: Randomly sampling data with specific batch size from the buffer\n",
    "        # Hint: you may use the python library \"random\".\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state  = []\n",
    "        action = []\n",
    "        reward = []\n",
    "        next_state = []\n",
    "        done = []\n",
    "        for sample in batch:\n",
    "            state.append(sample[0])\n",
    "            action.append(sample[1])\n",
    "            reward.append(sample[2])\n",
    "            next_state.append(sample[3])\n",
    "            done.append(sample[4])\n",
    "        \n",
    "        # If you are not familiar with the \"deque\" python library, please google it.\n",
    "        ######## YOUR CODE HERE! ########\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run DQN Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Wrapper.layers import *\n",
    "from Wrapper.wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "import math, random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# from dqn import QLearner, compute_td_loss, ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env)\n",
    "env = wrap_pytorch(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot observation of env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_frames = 1000000\n",
    "# batch_size = 32\n",
    "# gamma = 0.99\n",
    "    \n",
    "# replay_initial = 10000\n",
    "# replay_buffer = ReplayBuffer(100000)\n",
    "# model = QLearner(env, num_frames, batch_size, gamma, replay_buffer)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "# if USE_CUDA:\n",
    "#     model = model.cuda()\n",
    "\n",
    "# epsilon_start = 1.0\n",
    "# epsilon_final = 0.01\n",
    "# epsilon_decay = 30000\n",
    "# epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "# losses = []\n",
    "# all_rewards = []\n",
    "# episode_reward = 0\n",
    "\n",
    "# state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_frames = 1000000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "    \n",
    "replay_initial = 10000\n",
    "replay_buffer = ReplayBuffer(10000)\n",
    "model = QLearner(env, num_frames, batch_size, gamma, replay_buffer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Frame: 10000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 20000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 30000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 40000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 50000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 60000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 70000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 80000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 90000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 100000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 110000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 120000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 130000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 140000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 150000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 160000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 170000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 180000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 190000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 200000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 210000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 220000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 230000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 240000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 250000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 260000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 270000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 280000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 290000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 300000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 310000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 320000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 330000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 340000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 350000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 360000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 370000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 380000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 390000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 400000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 410000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 420000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 430000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 440000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 450000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 460000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 470000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 480000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 490000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 500000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 510000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 520000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 530000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 540000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 550000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 560000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 570000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 580000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 590000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 600000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 610000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 620000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 630000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 640000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 650000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 660000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 670000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 680000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 690000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 700000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 710000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 720000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 730000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 740000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 750000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 760000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 770000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 780000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 790000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 800000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 810000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 820000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 830000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 840000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 850000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 860000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 870000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 880000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 890000, preparing replay buffer\n",
      "10000\n",
      "#Frame: 900000, preparing replay buffer\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "for frame_idx in range(1, num_frames + 1):\n",
    "\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "\n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(model, batch_size, gamma, replay_buffer)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "    if frame_idx % 10000 == 0 and len(replay_buffer) <= replay_initial:\n",
    "        print('#Frame: %d, preparing replay buffer' % frame_idx)\n",
    "        print(len(replay_buffer))\n",
    "    if frame_idx % 10000 == 0 and len(replay_buffer) > replay_initial:\n",
    "        print('#Frame: %d, Loss: %f' % (frame_idx, np.mean(losses)))\n",
    "        print('Last-10 average reward: %f' % np.mean(all_rewards[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
