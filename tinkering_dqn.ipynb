{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ad65a81a95a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import math, random\n",
    "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "class QLearner(nn.Module):\n",
    "    def __init__(self, env, num_frames, batch_size, gamma, replay_buffer):\n",
    "        super(QLearner, self).__init__()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.num_frames = num_frames\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.env = env\n",
    "        self.input_shape = self.env.observation_space.shape\n",
    "        self.num_actions = self.env.action_space.n\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(self.input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.feature_size(), 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, self.num_actions)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    def feature_size(self):\n",
    "            return self.features(autograd.Variable(torch.zeros(1, *self.input_shape))).view(1, -1).size(1)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = Variable(torch.FloatTensor(np.float32(state)).unsqueeze(0), requires_grad=True)\n",
    "            ######## YOUR CODE HERE! ########\n",
    "            # TODO: Given state, you should write code to get the Q value and chosen action\n",
    "            # Complete the R.H.S. of the following 2 lines and uncomment them\n",
    "            # q_value =\n",
    "            # action =\n",
    "            ######## YOUR CODE HERE! ########\n",
    "        else:\n",
    "            action = random.randrange(self.env.action_space.n)\n",
    "        return action\n",
    "        \n",
    "def compute_td_loss(model, batch_size, gamma, replay_buffer):\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    state = Variable(torch.FloatTensor(np.float32(state)))\n",
    "    next_state = Variable(torch.FloatTensor(np.float32(next_state)), requires_grad=True)\n",
    "    action = Variable(torch.LongTensor(action))\n",
    "    reward = Variable(torch.FloatTensor(reward))\n",
    "    done = Variable(torch.FloatTensor(done))\n",
    "\n",
    "    ######## YOUR CODE HERE! ########\n",
    "    # TODO: Implement the Temporal Difference Loss\n",
    "    # loss =\n",
    "    ######## YOUR CODE HERE! ########\n",
    "    return loss\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)\n",
    "\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ######## YOUR CODE HERE! ########\n",
    "        # TODO: Randomly sampling data with specific batch size from the buffer\n",
    "        # Hint: you may use the python library \"random\".\n",
    "        # If you are not familiar with the \"deque\" python library, please google it.\n",
    "        ######## YOUR CODE HERE! ########\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run DQN Pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Wrapper.layers import *\n",
    "from Wrapper.wrappers import make_atari, wrap_deepmind, wrap_pytorch\n",
    "import math, random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "from dqn import QLearner, compute_td_loss, ReplayBuffer\n",
    "\n",
    "env_id = \"PongNoFrameskip-v4\"\n",
    "env = make_atari(env_id)\n",
    "env = wrap_deepmind(env)\n",
    "env = wrap_pytorch(env)\n",
    "\n",
    "num_frames = 1000000\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "    \n",
    "replay_initial = 10000\n",
    "replay_buffer = ReplayBuffer(100000)\n",
    "model = QLearner(env, num_frames, batch_size, gamma, replay_buffer)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "if USE_CUDA:\n",
    "    model = model.cuda()\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * frame_idx / epsilon_decay)\n",
    "\n",
    "losses = []\n",
    "all_rewards = []\n",
    "episode_reward = 0\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "\n",
    "for frame_idx in range(1, num_frames + 1):\n",
    "\n",
    "    epsilon = epsilon_by_frame(frame_idx)\n",
    "    action = model.act(state, epsilon)\n",
    "    \n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    state = next_state\n",
    "    episode_reward += reward\n",
    "    \n",
    "    if done:\n",
    "        state = env.reset()\n",
    "        all_rewards.append(episode_reward)\n",
    "        episode_reward = 0\n",
    "\n",
    "    if len(replay_buffer) > replay_initial:\n",
    "        loss = compute_td_loss(model, batch_size, gamma, replay_buffer)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.data.cpu().numpy())\n",
    "\n",
    "    if frame_idx % 10000 == 0 and len(replay_buffer) <= replay_initial:\n",
    "        print('#Frame: %d, preparing replay buffer' % frame_idx)\n",
    "\n",
    "    if frame_idx % 10000 == 0 and len(replay_buffer) > replay_initial:\n",
    "        print('#Frame: %d, Loss: %f' % (frame_idx, np.mean(losses)))\n",
    "        print('Last-10 average reward: %f' % np.mean(all_rewards[-10:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearnigEEG",
   "language": "python",
   "name": "deeplearnigeeg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
